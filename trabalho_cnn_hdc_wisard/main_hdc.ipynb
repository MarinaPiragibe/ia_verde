{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cfead85",
   "metadata": {},
   "source": [
    "## CNN: Fine-Tunning com HDC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93977e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e5039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import utils, globals\n",
    "import torch\n",
    "from modules import encoders\n",
    "from binhd.classifiers import BinHD\n",
    "from modules.cifake import Cifake\n",
    "import wisardpkg as wp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed3bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # # Download latest version\n",
    "# path = kagglehub.dataset_download(\"birdy654/cifake-real-and-ai-generated-synthetic-images\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6b71f",
   "metadata": {},
   "source": [
    "### Carregando o dataset pré-treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a651c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinapiragibe/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/marinapiragibe/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo 'resnet18_cifake_finetuned_float32.pth' carregado para avaliação de desempenho.\n",
      "Modelo pré-treinado carregado.\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = utils.load_model_from_file(\"resnet18_cifake_finetuned_float32.pth\")\n",
    "print(\"Modelo pré-treinado carregado.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d725bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import torchvision\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                     transforms.Resize(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                          std=[0.229, 0.224, 0.225])\n",
    "                  ])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=os.path.join(globals.DATASET_PATH, 'train'),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=os.path.join(globals.DATASET_PATH, 'test'),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Cria subsets para testar a lógica do modelo com um número menor do dataset (descomentar para usar)\n",
    "subset_train_indices = list(range(globals.NUM_SAMPLES_TRAIN_DEBUGGER))\n",
    "subset_test_indices = list(range(globals.NUM_SAMPLES_TEST_DEBUGGER))\n",
    "\n",
    "total_train_samples = len(train_dataset)\n",
    "num_train_to_select = min(globals.NUM_SAMPLES_TRAIN_DEBUGGER, total_train_samples)\n",
    "subset_train_indices = random.sample(range(total_train_samples), num_train_to_select)\n",
    "\n",
    "# Pega uma amostra aleatória de índices para o teste\n",
    "total_test_samples = len(test_dataset)\n",
    "num_test_to_select = min(globals.NUM_SAMPLES_TEST_DEBUGGER, total_test_samples)\n",
    "subset_test_indices = random.sample(range(total_test_samples), num_test_to_select)\n",
    "\n",
    "train_dataset_debugger = Subset(train_dataset, subset_train_indices)\n",
    "test_dataset_debugger = Subset(test_dataset, subset_test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset_debugger,\n",
    "    batch_size=globals.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset_debugger,\n",
    "    batch_size=globals.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3516e9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/13tensor([0, 0, 1, 1, 1, 1, 0, 0])\n",
      "2/13tensor([1, 0, 0, 0, 0, 1, 1, 1])\n",
      "3/13tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "4/13tensor([0, 0, 0, 1, 1, 0, 1, 0])\n",
      "5/13tensor([1, 0, 0, 1, 0, 1, 0, 1])\n",
      "6/13tensor([0, 0, 0, 0, 0, 1, 0, 0])\n",
      "7/13tensor([1, 1, 0, 0, 0, 1, 1, 1])\n",
      "8/13tensor([1, 0, 1, 1, 1, 1, 1, 0])\n",
      "9/13tensor([1, 0, 1, 1, 1, 1, 0, 1])\n",
      "10/13tensor([1, 0, 1, 0, 1, 0, 1, 0])\n",
      "11/13tensor([0, 0, 0, 0, 1, 1, 0, 1])\n",
      "12/13tensor([1, 0, 1, 0, 0, 0, 1, 1])\n",
      "13/13tensor([0, 1, 0, 0])\n",
      "[tensor([[2.2196, 1.3594, 1.1709,  ..., 0.4722, 1.2890, 0.9199],\n",
      "        [1.9494, 0.8896, 1.1723,  ..., 1.2122, 1.3804, 1.2615],\n",
      "        [0.8661, 0.8828, 1.8372,  ..., 0.4204, 1.4998, 1.5154],\n",
      "        ...,\n",
      "        [0.2653, 0.1763, 0.5505,  ..., 1.6606, 0.4659, 0.1547],\n",
      "        [0.7411, 1.3107, 0.1904,  ..., 0.7088, 0.6788, 0.7077],\n",
      "        [0.5788, 1.7012, 2.2310,  ..., 2.5163, 1.2418, 0.1205]]), tensor([[1.0687, 0.4493, 0.6851,  ..., 0.6031, 0.4936, 0.1024],\n",
      "        [0.0099, 0.6405, 0.9187,  ..., 1.7196, 0.8884, 0.3000],\n",
      "        [0.2120, 0.2854, 0.6052,  ..., 0.0776, 2.3508, 1.0544],\n",
      "        ...,\n",
      "        [1.4999, 0.9055, 0.1401,  ..., 0.8109, 0.1228, 0.4332],\n",
      "        [0.4044, 0.0038, 0.8922,  ..., 0.7028, 1.5358, 0.7138],\n",
      "        [1.2704, 0.1724, 0.8814,  ..., 1.4222, 0.4691, 0.0231]]), tensor([[1.1203, 1.0134, 0.3633,  ..., 1.0571, 0.1512, 2.5810],\n",
      "        [0.1699, 0.9974, 0.5323,  ..., 0.1275, 1.2183, 0.8558],\n",
      "        [1.3689, 1.0898, 0.4406,  ..., 1.3940, 1.1398, 0.2241],\n",
      "        ...,\n",
      "        [0.5685, 0.2103, 0.4938,  ..., 0.1864, 1.1344, 1.3080],\n",
      "        [2.4826, 1.0241, 2.4997,  ..., 0.0440, 0.2821, 1.4228],\n",
      "        [0.1846, 0.0435, 0.8810,  ..., 0.2004, 0.7278, 1.4705]]), tensor([[0.2800, 0.9135, 2.9174,  ..., 0.3169, 0.0457, 2.2398],\n",
      "        [0.9149, 0.1032, 3.4127,  ..., 0.1959, 4.7655, 2.4710],\n",
      "        [1.1366, 0.2150, 2.4952,  ..., 0.0566, 0.4151, 1.3492],\n",
      "        ...,\n",
      "        [1.5996, 1.3470, 1.5881,  ..., 1.7005, 2.0412, 1.9367],\n",
      "        [0.4919, 0.5478, 0.8559,  ..., 1.3602, 0.1068, 0.9185],\n",
      "        [1.3070, 1.3386, 1.5929,  ..., 0.1127, 1.3001, 1.3931]]), tensor([[0.6049, 0.4453, 1.0149,  ..., 2.5881, 0.5304, 0.1037],\n",
      "        [2.7019, 0.3327, 1.5798,  ..., 0.0824, 3.1119, 0.5893],\n",
      "        [0.0545, 1.8116, 0.5536,  ..., 0.1408, 0.9566, 2.2737],\n",
      "        ...,\n",
      "        [0.0722, 0.2186, 1.2737,  ..., 0.4413, 1.4013, 0.3448],\n",
      "        [1.1507, 0.7063, 0.1305,  ..., 1.8648, 0.6078, 0.6018],\n",
      "        [0.2651, 1.3654, 1.4342,  ..., 0.7221, 0.0737, 0.7004]]), tensor([[1.4022, 0.7707, 2.6604,  ..., 0.7992, 0.3128, 0.9989],\n",
      "        [2.3287, 0.9257, 1.2231,  ..., 0.3601, 3.4069, 0.8779],\n",
      "        [1.3256, 1.0668, 0.7099,  ..., 0.3189, 1.3282, 0.4345],\n",
      "        ...,\n",
      "        [1.5487, 0.2722, 0.5725,  ..., 2.3438, 0.7316, 0.5034],\n",
      "        [1.0899, 0.9123, 1.9617,  ..., 1.0783, 0.7456, 0.3179],\n",
      "        [0.8350, 1.7538, 0.5942,  ..., 0.1016, 1.7382, 1.7283]]), tensor([[1.0757, 0.0505, 0.4448,  ..., 1.7107, 0.8295, 0.6987],\n",
      "        [2.0155, 0.4485, 0.9561,  ..., 1.7672, 0.9895, 0.2910],\n",
      "        [0.3188, 0.3471, 2.7473,  ..., 0.1353, 0.0976, 1.6089],\n",
      "        ...,\n",
      "        [0.8795, 0.2076, 1.1099,  ..., 1.1097, 0.9053, 0.2317],\n",
      "        [0.8916, 0.1359, 0.2590,  ..., 1.4398, 0.2468, 0.1269],\n",
      "        [0.5223, 0.8675, 2.2369,  ..., 0.1921, 0.2280, 0.1995]]), tensor([[0.6045, 1.3450, 0.1172,  ..., 0.1735, 2.4806, 0.5111],\n",
      "        [0.3227, 0.8883, 0.3603,  ..., 0.2941, 1.8330, 0.7940],\n",
      "        [0.4005, 1.7306, 2.1083,  ..., 1.1142, 0.2014, 0.8661],\n",
      "        ...,\n",
      "        [0.3090, 0.6864, 0.0689,  ..., 0.4009, 0.8376, 0.1741],\n",
      "        [0.3209, 0.4845, 1.2047,  ..., 0.2378, 0.4137, 1.0383],\n",
      "        [0.6138, 1.0277, 1.0983,  ..., 1.1750, 1.0668, 1.4398]]), tensor([[0.4871, 2.0559, 0.5876,  ..., 4.3587, 1.0698, 2.7473],\n",
      "        [0.3140, 0.4294, 0.4113,  ..., 0.2429, 1.8903, 0.8222],\n",
      "        [0.4476, 0.1071, 0.3604,  ..., 2.0986, 0.0113, 0.1293],\n",
      "        ...,\n",
      "        [0.3547, 0.2000, 0.5371,  ..., 1.9380, 0.8845, 0.2630],\n",
      "        [0.3629, 0.1585, 1.1272,  ..., 0.4160, 1.7893, 0.1168],\n",
      "        [0.4525, 1.3499, 0.3711,  ..., 0.8866, 1.6879, 0.7956]]), tensor([[0.7584, 1.1224, 0.5688,  ..., 1.3157, 0.5344, 1.1860],\n",
      "        [0.7915, 1.0906, 0.6711,  ..., 0.5757, 1.2282, 1.6111],\n",
      "        [1.5704, 0.5911, 0.7255,  ..., 0.4013, 0.6690, 1.0067],\n",
      "        ...,\n",
      "        [1.3335, 1.1030, 1.4829,  ..., 0.1327, 3.3553, 0.9403],\n",
      "        [0.0550, 1.2726, 0.9148,  ..., 0.4773, 2.2624, 0.3370],\n",
      "        [0.1330, 0.2563, 1.8579,  ..., 0.2280, 1.7685, 0.2179]]), tensor([[0.0185, 2.4162, 0.2405,  ..., 0.0296, 0.2520, 1.1576],\n",
      "        [0.1640, 1.7352, 1.8166,  ..., 0.3992, 1.9998, 0.5293],\n",
      "        [0.7967, 0.3383, 1.6707,  ..., 0.3508, 1.2223, 0.5698],\n",
      "        ...,\n",
      "        [1.0772, 0.5677, 0.9586,  ..., 0.9127, 1.8127, 1.4401],\n",
      "        [0.1303, 0.5633, 0.2684,  ..., 0.3972, 0.4245, 1.2833],\n",
      "        [1.3245, 2.0957, 0.0463,  ..., 1.7791, 0.8074, 0.8387]]), tensor([[0.1439, 0.8569, 1.3919,  ..., 2.0121, 0.2130, 0.9927],\n",
      "        [0.5864, 0.1986, 1.9521,  ..., 0.0039, 3.2480, 1.6290],\n",
      "        [0.0959, 1.6359, 1.6515,  ..., 0.2915, 0.3458, 0.0294],\n",
      "        ...,\n",
      "        [0.9118, 0.5699, 0.3245,  ..., 0.0674, 1.0183, 0.6618],\n",
      "        [1.0886, 0.1298, 0.3348,  ..., 1.6019, 1.6659, 0.3632],\n",
      "        [2.1854, 2.3591, 0.9117,  ..., 0.6182, 0.6848, 3.7393]]), tensor([[1.2226, 1.1284, 2.5072,  ..., 0.1069, 0.0901, 1.7687],\n",
      "        [0.2351, 0.1342, 0.3297,  ..., 0.0779, 2.2917, 1.5173],\n",
      "        [0.0140, 0.0629, 1.1087,  ..., 0.0734, 0.3420, 0.2470],\n",
      "        [0.3523, 0.9948, 2.0936,  ..., 0.4350, 0.3803, 1.0590]])]\n",
      "[tensor([0, 0, 1, 1, 1, 1, 0, 0]), tensor([1, 0, 0, 0, 0, 1, 1, 1]), tensor([1, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 1, 1, 0, 1, 0]), tensor([1, 0, 0, 1, 0, 1, 0, 1]), tensor([0, 0, 0, 0, 0, 1, 0, 0]), tensor([1, 1, 0, 0, 0, 1, 1, 1]), tensor([1, 0, 1, 1, 1, 1, 1, 0]), tensor([1, 0, 1, 1, 1, 1, 0, 1]), tensor([1, 0, 1, 0, 1, 0, 1, 0]), tensor([0, 0, 0, 0, 1, 1, 0, 1]), tensor([1, 0, 1, 0, 0, 0, 1, 1]), tensor([0, 1, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for k, batch in enumerate(train_loader):\n",
    "        print(f'\\r{k+1}/{len(train_loader)}', end='', flush=True) \n",
    "\n",
    "        dado, rotulo = batch\n",
    "        dado = dado.to(globals.DEVICE)\n",
    "        rotulo = rotulo.to(globals.DEVICE)\n",
    "        print(rotulo)\n",
    "        # Extrair features\n",
    "        features = feature_extractor(dado)\n",
    "\n",
    "        # Achatar (flatten) o tensor para (batch_size, num_features)\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        all_features.append(features.cpu())\n",
    "        all_labels.append(rotulo)\n",
    "\n",
    "print(all_features)\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90690e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0\n",
      " 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
      " 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0]\n",
      "Features salvas em 'features.npy' e labels em 'labels.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "features_array = np.concatenate(all_features, axis=0)\n",
    "labels_array = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "print(labels_array)\n",
    "\n",
    "\n",
    "np.save('features.npy', features_array)\n",
    "np.save('labels.npy', labels_array)\n",
    "\n",
    "print(\"Features salvas em 'features.npy' e labels em 'labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a25bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de samples: 100\n",
      "Classes encontradas:       feat_0    feat_1    feat_2    feat_3    feat_4    feat_5    feat_6  \\\n",
      "0   2.219618  1.359371  1.170898  0.466075  0.131603  1.496855  0.984377   \n",
      "1   1.949351  0.889635  1.172312  0.854865  0.395240  0.961098  1.311359   \n",
      "2   0.866102  0.882795  1.837225  1.502132  1.152856  0.596776  1.798405   \n",
      "3   0.999963  2.078024  0.550981  1.369875  3.662720  0.750793  0.352376   \n",
      "4   0.573586  0.893467  2.847860  1.423239  2.952774  0.403388  1.044446   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "95  2.185391  2.359076  0.911664  2.610189  1.880452  0.662483  0.185209   \n",
      "96  1.222568  1.128433  2.507212  0.214144  1.139926  0.901808  1.023088   \n",
      "97  0.235074  0.134231  0.329738  0.666386  1.227254  0.456718  1.326144   \n",
      "98  0.014018  0.062944  1.108675  1.830286  0.178609  0.074424  2.486744   \n",
      "99  0.352333  0.994777  2.093576  1.099472  0.989623  1.942674  1.075790   \n",
      "\n",
      "      feat_7    feat_8    feat_9  ...  feat_502  feat_503  feat_504  feat_505  \\\n",
      "0   2.538312  0.308608  1.411603  ...  0.635127  0.584593  1.328055  1.582227   \n",
      "1   1.300301  1.119074  0.757776  ...  0.447892  0.964375  0.554088  1.901782   \n",
      "2   0.021624  1.055051  0.705358  ...  2.067196  2.967087  0.757346  2.336833   \n",
      "3   0.344511  0.654607  0.754411  ...  0.990012  0.811182  0.783209  0.799385   \n",
      "4   0.882424  0.265327  0.976178  ...  2.324897  0.223327  0.864664  0.766258   \n",
      "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
      "95  3.297050  1.892663  0.026924  ...  0.012879  0.452731  0.132007  1.085959   \n",
      "96  0.278173  0.827810  1.492055  ...  0.503039  1.641022  0.863071  0.313772   \n",
      "97  0.991291  0.081501  0.263307  ...  1.391186  0.601142  0.017217  1.244388   \n",
      "98  0.654144  0.701111  2.360737  ...  1.029127  0.401947  1.675749  0.706090   \n",
      "99  1.052753  0.379048  1.527163  ...  0.036722  0.478961  0.485878  0.440311   \n",
      "\n",
      "    feat_506  feat_507  feat_508  feat_509  feat_510  feat_511  \n",
      "0   0.317405  0.284001  0.769318  0.472227  1.288987  0.919934  \n",
      "1   0.155587  1.367975  0.315222  1.212167  1.380373  1.261524  \n",
      "2   1.114857  0.759990  2.665053  0.420437  1.499788  1.515447  \n",
      "3   0.107774  0.830232  1.222561  1.333666  0.916385  0.179958  \n",
      "4   0.001739  1.715951  1.895404  3.765793  1.136225  0.645215  \n",
      "..       ...       ...       ...       ...       ...       ...  \n",
      "95  0.470758  0.242472  0.639493  0.618221  0.684766  3.739294  \n",
      "96  0.668722  1.885015  1.149000  0.106944  0.090079  1.768700  \n",
      "97  0.059969  0.909847  0.990245  0.077921  2.291743  1.517269  \n",
      "98  0.746602  1.105650  1.200778  0.073406  0.342023  0.247017  \n",
      "99  0.682103  1.283089  1.896464  0.435027  0.380275  1.058969  \n",
      "\n",
      "[100 rows x 512 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "cifake = Cifake()\n",
    "# Verifica as amostras e classes\n",
    "print(f\"Número de samples: {len(cifake.samples)}\")\n",
    "print(f\"Classes encontradas: {cifake.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f3599a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 7.589083194732666\n"
     ]
    }
   ],
   "source": [
    "min_val, max_val = cifake.get_min_max_values()\n",
    "print(min_val, max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e1f5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 1000\n",
    "num_levels = 500\n",
    "batch_size = 100\n",
    "low = 0\n",
    "high = num_levels\n",
    "oper = \"bind\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff67439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     0\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "     ..\n",
      "95    1\n",
      "96    0\n",
      "97    1\n",
      "98    0\n",
      "99    0\n",
      "Name: class, Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "from modules.encoders import RecordEncoder\n",
    "\n",
    "X = cifake.features\n",
    "\n",
    "y = cifake.labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = torch.tensor(le.fit_transform(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d5d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinHD(dimension, cifake.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de23842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_encoder = RecordEncoder(\n",
    "            out_features=dimension,\n",
    "            size=X.shape[1], \n",
    "            levels=num_levels,\n",
    "            low=low,\n",
    "            high=high\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "444a99ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38749/3094186489.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_encoded = torch.tensor(y_encoded).to(globals.DEVICE)\n"
     ]
    }
   ],
   "source": [
    "y_encoded = torch.tensor(y_encoded).to(globals.DEVICE)\n",
    "\n",
    "def run_encoders(X, y, device):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(X.dtypes)\n",
    "\n",
    "        samples = torch.tensor(X.values).to(device)\n",
    "\n",
    "        # RecordEncoder\n",
    "        return record_encoder(samples.detach().clone())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e54edbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_0      float32\n",
      "feat_1      float32\n",
      "feat_2      float32\n",
      "feat_3      float32\n",
      "feat_4      float32\n",
      "             ...   \n",
      "feat_507    float32\n",
      "feat_508    float32\n",
      "feat_509    float32\n",
      "feat_510    float32\n",
      "feat_511    float32\n",
      "Length: 512, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_record_encoder = run_encoders(X, y, globals.DEVICE)\n",
    "labels = torch.tensor(y).to(globals.DEVICE)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_record_encoder, labels, test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecd0ad45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinHD Record Encoder: Accuracy =  0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.fit(X_train,y_train)\n",
    "    predictions = model.predict(X_test.to(torch.int8))  \n",
    "    acc = accuracy_score(predictions, y_test)\n",
    "    print(\"BinHD Record Encoder: Accuracy = \", acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
